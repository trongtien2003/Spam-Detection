# PhoBERT.yaml
model_variants:
  phobert_base:
    batch_size: 16
    epochs: 3
    learning_rate: 2e-5
    patience: 5
    warmup_steps_ratio: 0.1
    max_length: 128
    force_preprocess: False

  phobert_large:
    batch_size: 8
    epochs: 5
    learning_rate: 3e-5
    patience: 3
    warmup_steps_ratio: 0.2
    max_length: 256
    force_preprocess: True

  phobert_tuned_1:
    batch_size: 32
    epochs: 4
    learning_rate: 1e-5
    patience: 4
    warmup_steps_ratio: 0.15
    max_length: 128
    force_preprocess: False

  phobert_tuned_2:
    batch_size: 16
    epochs: 6
    learning_rate: 5e-6
    patience: 6
    warmup_steps_ratio: 0.1
    max_length: 512
    force_preprocess: True
